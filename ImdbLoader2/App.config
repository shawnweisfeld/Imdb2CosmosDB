<?xml version="1.0" encoding="utf-8"?>
<configuration>
    <startup> 
        <supportedRuntime version="v4.0" sku=".NETFramework,Version=v4.5.2" />
    </startup>
  <appSettings file="LocalApp.config">

    <!-- You must fill in these keys -->
    
    <!-- Azure Storage account where we will drop files during the process -->
    <add key="StorageAccountConnectionString" value="" />
    
    <!-- The Cosmos DB to load -->
    <add key="GraphEndpoint" value="" />
    <add key="GraphKey" value="" />
    <add key="GraphDatabase" value="" />

    
    <!-- An Azure Batch job used to load the DB -->
    <add key="BatchAccountName" value="" />
    <add key="BatchAccountKey" value="" />
    <add key="BatchAccountUrl" value="" />
    
  
    <!-- You can alter these keys depending on your needs -->
      
    <!-- this is the FTP site for the IMDB source file. We need the Actors.list -->
    <!-- view full terms and conditions and additional FTP sites here: http://www.imdb.com/interfaces -->
    <add key="ImdbSourceFile" value="ftp://ftp.fu-berlin.de/pub/misc/movies/database/actors.list.gz" />
    
    <!-- Location to cache the compressed IMDB source file, to eliminate needs to FTP it down again if we have to rerun -->
    <add key="ImdbSourceFileContainer" value="imdbsourcefile" />
    
    <!-- Location to put the parsed files, after we decompress the IMDB source file and parse it, while they wait for Azure Batch to load them -->
    <add key="ImdbParsedFileContainer" value="imdbparsedfile" />

    <!-- This will control how often updates are sent to the console when parsing the imdb flat file, in number of records -->
    <add key="FeedbackFrequency" value="100000" />
    
    <!-- This will set the size of the parsed files that we create from the imdb flat file, in number of records -->
    <add key="BatchSize" value="100000" />
    
    <!-- This will truncate the imdb flat file after this many rows are injested, good for testing smaller batches, set to -1 to process the entire file -->
    <add key="RowsToLoad" value="-1" />

    <!-- this is multiplied by the number of processors in the machine to get total Max DOP-->
    <add key="MaxDegreeOfParallelism" value="8"/>

    <!-- New collection to create to put the data in -->
    <add key="GraphCollection" value="imdb" />
    
    <!-- How many RUs should the whole collectio have total. -->
    <add key="GraphThroughput" value="100000" />
    
    <!-- How many open HTTP connections should we allow when talking to Cosmos -->
    <add key="ConnectionsPerProc" value="500" />

    <!-- storage container to put the binaries for the batch job -->
    <add key="BatchAppStorageContainer" value="batchapp" />
    
    <!-- Name of the Batch Job/Pool -->
    <add key="BatchJobId" value="ImdbJob" />
    <add key="BatchPoolId" value="ImdbPool" />

    <!-- How many and what type of compute nodes should the batch job use. -->
    <!-- you want to make sure that you have enough to process the data fast, but not too many that you swamp the Cosmos DB -->
    <add key="ComputeNodes" value="30" />
    <add key="VirtualMachineSize" value="Standard_D1_v2" />

  </appSettings>
  <runtime>
    <assemblyBinding xmlns="urn:schemas-microsoft-com:asm.v1">
      <dependentAssembly>
        <assemblyIdentity name="Microsoft.Azure.KeyVault.Core" publicKeyToken="31bf3856ad364e35" culture="neutral" />
        <bindingRedirect oldVersion="0.0.0.0-2.0.0.0" newVersion="2.0.0.0" />
      </dependentAssembly>
      <dependentAssembly>
        <assemblyIdentity name="Newtonsoft.Json" publicKeyToken="30ad4fe6b2a6aeed" culture="neutral" />
        <bindingRedirect oldVersion="0.0.0.0-10.0.0.0" newVersion="10.0.0.0" />
      </dependentAssembly>
    </assemblyBinding>
  </runtime>
</configuration>